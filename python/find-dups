#!/usr/bin/env python
"""
Given a root directory, recurse in it and find all the duplicate
files, files that have the same contents, but not necessarily the
same filename.
"""

# created by David Mertz and Martin Blais
#
# This code is released as CC-0
# http://creativecommons.org/publicdomain/zero/1.0/
#
# The person who associated a work with this deed has dedicated the work to
# the public domain by waiving all of his or her rights to the work
# worldwide under copyright law, including all related and neighboring
# rights, to the extent allowed by law.
#
# You can copy, modify, distribute and perform the work, even for
# commercial purposes, all without asking permission. See Other Information
# below.

from sys import maxsize, stderr, stdout
from os import remove, walk, readlink, cpu_count, scandir
from os.path import islink, abspath
from fnmatch import fnmatch
from os.path import isdir, exists, join, isfile, getsize
from hashlib import sha1
from itertools import groupby
from multiprocessing import Pool
import sqlite3


def main():
    import optparse
    parser = optparse.OptionParser(__doc__.strip())
    parser.add_option('--size-only', type="int", default=maxsize,
                      help="(Debugging) consider files a match on size "
                           "alone if they are larger than SIZE_ONLY")
    parser.add_option('--use-checkpoint', action="store_true",
                      default=False,
                      help="Begin with stored intermediate results "
                           "(perhaps out of date)")
    parser.add_option('-M', '--max-size', type="int", default=maxsize,
                      help="Ignore files larger than MAX_SIZE")
    parser.add_option('-m', '--min-size', type="int", default=0,
                      help="Ignore files smaller than MIN_SIZE")
    parser.add_option('-i', '--ignore-symlinks', action="store_true",
                       default=False,
                       help="Do not treat symbolic links as duplicates")
    parser.add_option('-p', '--parallel', action="store_true",
                      default=False,
                      help="Attempt to use parallel SHA1 calculations")
    parser.add_option('-s', '--scandir', action="store_true",
                      default=False,
                      help="Use os.scandir() rather than os.walk()")
    parser.add_option('-g', '--glob', type="str", default="*",
                      help="Limit matches to glob pattern")
    parser.add_option('-v', '--verbose', action="store_true",
                      default=False,
                      help="Display progress information on STDERR")
    opts, args = parser.parse_args()
    if not args:
        parser.error("You must specify directories to search.")

    # If we are using parallel style, create a pool once
    if opts.parallel:
        opts.parallel = Pool(processes=int(cpu_count()*0.75))
    find_duplicates(args, opts)


def find_files(args, pat='*'):
    for dir in args:
        if isdir(dir):
            for root, dirs, fnames in walk(dir):
                for f in fnames:
                    if fnmatch(f, pat):
                        yield join(root, f)
        else:
            yield dir

def scan_files(args, opts):
    for dir in args:
        if isdir(dir):
            for entry in scandir(dir):
                if entry.is_dir(follow_symlinks=False):
                    yield from scan_files([entry.path], opts)
                elif entry.is_file(follow_symlinks=not opts.ignore_symlinks):
                    if fnmatch(entry.name, opts.glob):
                        try:
                            size = entry.stat().st_size
                            yield (entry.path, size)
                        except FileNotFoundError as err:
                            if opts.verbose:
                                print(err, file=stderr)

def subdirs(args):
    """Yield directory names not starting with '.' under given path."""
    for entry in os.scandir(path):
        if entry.is_dir():
            yield entry.name


def hash_content(f):
    if islink(f):
        # No need to spend SHA1 time on symlink
        return ("_SYMLINK" , f)
    try:
        with open(f, 'rb') as fh:
            content = fh.read()
            return (sha1(content).hexdigest(), f)
    except IOError as s:
        print(s, file=stderr)
        return ("_ERROR", f)


def parallel_hash(fnames, pool=None):
    if pool is none:
        # Shouldn't hit this path, but just in case...
        pool = Pool(processes=cpu_count()//2)
    return pool.map(hash_content, fnames)


def group_pairs(pairs):
    """Combine adjacent second items in pairs

    This function is passed an interable each of whose values is a pair;
    it yields a sequence of pairs whose first element is the identical
    first element from the original pairs, and whose second element is a
    list of second elements corresponding to the same first element. Only
    adjacent pairs sharing a first element are grouped together, so if the
    grouping is required to be global, you should pass in 'sorted(pairs)'
    rather than the raw iterable.  E.g.:

    >>> things = [(1,'foo'), (1,'bar'), (2, 'baz')]
    >>> group_pairs(things)
    [(1,['foo','bar']), (2, ['baz'])]
    """
    for idx, vals in groupby(pairs, lambda pair: pair[0]):
        yield (idx, [v[1] for v in vals])


def find_duplicates(dirs, opts):
    "Find the duplicate files in the given root directory."

    # Organize all filenames according to size.
    if opts.use_checkpoint and exists('sz.db'):
        conn = sqlite3.connect('sz.db')
        c = conn.cursor()
        if opts.verbose:
            print("Read cached file sizes from 'sz.db'...", file=stderr)
    else:
        count = 0
        if exists('sz.db'):
            remove('sz.db')
        conn = sqlite3.connect('sz.db')
        c = conn.cursor()
        c.execute('create table files_by_size (size int, fname text)')
        if opts.verbose:
            print("Checking sizes (each '.' is 100k files):", file=stderr)
        if opts.scandir:
            for fn, sz in scan_files(dirs, opts):
                count += 1
                c.execute('insert into files_by_size values (?, ?)', (sz, fn))
                if opts.verbose and count % 100000 == 0:
                    stderr.write('.')
        else:
            for fn in find_files(dirs, opts.glob):
                if not isfile(fn):
                    continue
                count += 1
                sz = getsize(fn)
                c.execute('insert into files_by_size values (?, ?)', (sz, fn))
                if opts.verbose and count % 100000 == 0:
                    stderr.write('.')
        conn.commit()
        if opts.verbose:
            print("\nFound sizes on %d files..." % count, file=stderr)

    c.execute('''select size, fname from files_by_size
                 where size<=? and size>=?
                 order by size desc''', (opts.max_size, opts.min_size))

    if opts.verbose:
        print("Grouping files by SHA1 (each '.' is 5k groups):", file=stderr)

    distincts = 0
    null_header = False
    empties = 0
    for sz, fnames in group_pairs(c):
        # We might wish to exclude symbolic links
        if opts.ignore_symlinks:
            fnames = [f for f in fnames if not islink(f)]

        if sz == 0:
            if not null_header:
                print("Size: 0 | Content: ''")
                null_header = True
            for f in fnames:
                print(' ', f)
                empties += 1
        else:
            # We have accumulated some dups that need to be printed
            if len(fnames) > 1:
                if sz <= opts.size_only:
                    hashes = [hash_content(f) for f in fnames]
                elif opts.parallel:
                    hashes = parallel_hash(fnames, pool=opts.parallel)
                else:
                    hashes = []  # Empty hashes, report handled here
                    print('Size:', sz, '| Size:', sz)
                    for f in fnames:
                        print(' ', f)
                    distincts += 1

                for idx, vals in group_pairs(sorted(hashes)):
                    if len(vals) > 1:
                        distincts +=1
                        if sz > 40:
                            print('Size:', sz, '| SHA1:', idx)
                        else:
                            with open(vals[0], 'rb') as fh:
                                content = fh.read()
                            print('Size:', sz, '| Content:', repr(content))
                        for fn2 in vals:
                            if islink(fn2):
                                ln = "-> " + readlink(fn2)
                                print(' ', abspath(fn2), ln)
                            else:
                                print(' ', abspath(fn2))
                        if opts.verbose and distincts % 5000 == 0:
                            stderr.write('.')

    if opts.verbose:
        print("\nFound %d empty files"% empties, file=stderr)
        print("Found %d non-empty duplicate sets" % distincts, file=stderr)


if __name__ == '__main__':
    main()

