#!/usr/bin/env python
"""
Given a root directory, recurse in it and find all the duplicate
files, files that have the same contents, but not necessarily the
same filename.
"""

# created by David Mertz and Martin Blais
#
# This code is released as CC-0
# http://creativecommons.org/publicdomain/zero/1.0/
#
# The person who associated a work with this deed has dedicated the work to
# the public domain by waiving all of his or her rights to the work
# worldwide under copyright law, including all related and neighboring
# rights, to the extent allowed by law.
#
# You can copy, modify, distribute and perform the work, even for
# commercial purposes, all without asking permission. See Other Information
# below.

from sys import maxsize, stderr
from os import remove, readlink, cpu_count, scandir
from os.path import islink, abspath
from fnmatch import fnmatch
from os.path import isdir, exists
from hashlib import sha1
from itertools import groupby
from collections import namedtuple
from operator import itemgetter
from multiprocessing import Pool
import sqlite3
from pprint import pprint

# Counts of actions for debug/verbose mode (i.e. calculating hashes)
performed, copied = 0, 0

# Keep together associated file information
Finfo = namedtuple("Finfo", ["size", "path", "inode"])
Status = namedtuple("Status", ["hashes_performed", "hashes_copied"])


# I name my debug-print as XXX
def XXX(arg):
    pprint(arg, stream=stderr)


def main():
    import optparse
    parser = optparse.OptionParser(__doc__.strip())
    parser.add_option('--use-checkpoint', action="store_true",
                      default=False,
                      help="Begin with stored intermediate results "
                           "(perhaps out of date)")
    parser.add_option('-M', '--max-size', type="int", default=maxsize,
                      help="Ignore files larger than MAX_SIZE")
    parser.add_option('-m', '--min-size', type="int", default=1,
                      help="Ignore files smaller than MIN_SIZE")
    parser.add_option('-l', '--enable-symlinks', action="store_true",
                       default=False,
                       help="Include symlinks in duplication report")
    parser.add_option('-g', '--glob', type="str", default="*",
                      help="Limit matches to glob pattern")
    parser.add_option('-v', '--verbose', action="store_true",
                      default=False,
                      help="Display progress information on STDERR")
    opts, args = parser.parse_args()
    if not args:
        parser.error("You must specify directories to search.")

    find_duplicates(args, opts)


def scan_files(args, opts):
    for dir in args:
        if isdir(dir):
            for entry in scandir(dir):
                if entry.is_dir(follow_symlinks=opts.enable_symlinks):
                    yield from scan_files([entry.path], opts)
                elif entry.is_file(follow_symlinks=opts.enable_symlinks):
                    if fnmatch(entry.name, opts.glob):
                        try:
                            path = entry.path
                            size = entry.stat().st_size
                            inode = entry.inode()
                            yield Finfo(path, size, inode)
                        except FileNotFoundError as err:
                            if opts.verbose:
                                print(err, file=stderr)


def hash_content(finfo):
    try:
        with open(finfo.path, 'rb') as fh:
            content = fh.read()
            return (sha1(content).hexdigest(), finfo.path)
    except IOError as s:
        print(s, file=stderr)
        return ("_ERROR", finfo.path)


def parallel_hash(finfos, pool=None):
    global performed, copied
    uniq_inodes = [f[0] for _, f in group_by_key(finfos, 2, Finfo)
                        if len(f) == 1]
    dup_inodes = [f for _, f in group_by_key(finfos, key=2)
                    if len(f) > 1]

    # Use the pool to parallelize distinct inodes
    hashes = pool.map(hash_content, uniq_inodes)
    performed += len(hashes)

    # Might add to hashes if we have hardlink sets
    # Note: there COULD be many such inode sets, which are calculated
    #     serially.  However, the performance difference between serial
    #     and parallel is so small that it matters little.
    for dup_inode in dup_inodes:
        finfo = Finfo(*dup_inode[0])  # Use the first one
        digest, _ = hash_content(finfo)
        more_hashes = [(digest, dup[1]) for dup in dup_inode]
        performed += 1
        copied += len(dup_inode[1:])
        hashes.extend(more_hashes)

    return hashes, Status(performed, copied)


def group_by_key(records,
                 key=0, val_type=lambda *x: tuple(x), sort=False):
    """Combine adjacent second items in pairs

    This function is passed an interable each of whose values is a pair;
    it yields a sequence of pairs whose first element is the identical
    first element from the original pairs, and whose second element is a
    list of tail elements corresponding to the same first element. Only
    adjacent pairs sharing a first element are grouped together, so if the
    grouping is required to be global, you should pass in 'sorted(pairs)'
    rather than the raw iterable.  E.g.:

      >>> things = [(1,'foo', 17), (1,'bar', 119), (2, 'baz', 43)]
      >>> list(group_by_key(things))
      [(1, [(1, 'foo', 17), (1, 'bar', 119)]), (2, [(2, 'baz', 43)])]
      >>> Finfo = namedtuple("Finfo", ["size", "path", "inode"])
      >>> list(group_by_key(things, val_type=Finfo))
      [(1, [Finfo(size=1, path='foo', inode=17),
            Finfo(size=1, path='bar', inode=119)]),
       (2, [Finfo(size=2, path='baz', inode=43)])]
    """
    if sort < 0:
        records = sorted(records, key=itemgetter(key), reversed=True)
    elif sort > 0:
        records = sorted(records, key=itemgetter(key))

    for idx, vals in groupby(records, itemgetter(key)):
        yield (idx, [val_type(*v) for v in vals])


def db_connection(dirs, opts):
    # Organize all filenames according to size.
    if opts.use_checkpoint and exists('sz.db'):
        conn = sqlite3.connect('sz.db')
        cursor = conn.cursor()
        if opts.verbose:
            print("Read cached file sizes from 'sz.db'...", file=stderr)
    else:
        count = 0
        if exists('sz.db'):
            remove('sz.db')
        conn = sqlite3.connect('sz.db')
        cursor = conn.cursor()
        sql = 'CREATE TABLE files_by_size (size INT, fname TEXT, inode INT)'
        cursor.execute(sql)
        if opts.verbose:
            print("Checking sizes (each '.' is 100k files):", file=stderr)
        sql_insert = 'INSERT INTO files_by_size values (?, ?, ?)'
        for fn, sz, inode in scan_files(dirs, opts):
            count += 1
            cursor.execute(sql_insert, (sz, fn, inode))
            if opts.verbose and count % 100000 == 0:
                stderr.write('.')
        conn.commit()
        if opts.verbose:
            print("\nFound sizes on %d files..." % count, file=stderr)

    cursor.execute('''SELECT size, fname, inode FROM files_by_size
                      WHERE size<=? AND size>=?
                      ORDER BY size DESC''',
                   (opts.max_size, opts.min_size))
    return cursor


def find_duplicates(dirs, opts):
    "Find the duplicate files in the given root directory."
    cursor = db_connection(dirs, opts)

    if opts.verbose:
        print("Grouping files by SHA1 (each '.' is 5k groups):", file=stderr)

    # Need process pool
    pool = Pool(processes=int(cpu_count()*0.75))
    distincts = 0

    for sz, finfos in group_by_key(cursor, 0, Finfo):
        # We have accumulated some dups that need to be printed
        if len(finfos) > 1:
            hashes, st = parallel_hash(finfos, pool=pool)
            for hash, vals in group_by_key(hashes, sort=True):
                if len(vals) > 1:
                    distincts += 1
                    print('Size:', sz, '| SHA1:', hash)
                    for _, path in vals:
                        if islink(path):
                            ln = "-> " + readlink(path)
                            print(' ', abspath(path), ln)
                        else:
                            print(' ', abspath(path))
                    if opts.verbose and distincts % 5000 == 0:
                        stderr.write('.')

    if opts.verbose:
        print(file=stderr)
        print(f"Found     {distincts:,} duplicate sets", file=stderr)
        print(f"Performed {st.hashes_performed:,} SHA1 hashes", file=stderr)
        print(f"Copied    {st.hashes_copied:,} hardlink hashes", file=stderr)


if __name__ == '__main__':
    main()
